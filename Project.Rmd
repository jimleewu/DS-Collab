---
title: "Tesla Supercharger Analysis"
author: "Jim-Lee Wu and Fion Huang"
subtitle: 
output:
  pdf_document:
    number_sections: true
  html_document:
    number_sections: true
  word_document: default
---

```{r}
library(dplyr)
library(tidyverse)
library(tidymodels)
library(rsample)
library(corrplot)
library(ranger)
library(glmnet)
library(kknn)
library(xgboost)
library(ggplot2)
```

# Introduction

There has been an increasing rise in popularity in electronic vehicles (EVs) throughout the past decade for its environmental, economic, and performance benefits. With so many EVs filling the streets, a need for a proper EV charging infrastructure is needed. Just like gasoline cars, EV drivers require charging stations where they can recharge when going long distances.

# Dataset

The data set that we will be working on in this project is called `tesla`. The data consist of all cities from highly populated counties in California. The counties in our data set consist of Alameda, Contra Costa, Fresno, Kern, Los Angeles, Orange, Riverside, Sacramento, San Diego, San Francisco, San Joaquin, San Mateo, Santa Barbara, Santa Clara, Stanislaus, and Ventura. We collected demographic data and Tesla Supercharging data for each city in the before mention counties.

## Parameters

`city` City

`county` County of City

`population` 2022 Population

`land_size` 2020 Land area in square miles

`medium_income` medium household income in 2018-2022 (in 2022 dollars)

`per_capita_income` per_capita_income: mean income of every person in 2018-2022 (in 2022 dollars)

`registered_voter` the number of registered voters (March 11, 2022)

`democratic` the number of registered voters declared as democratic (March 11, 2022)

`republican`the number of registered voters declared as republican (March 11, 2022)

`no_stations` the number of Tesla Supercharger stations

`mile` the number of stations that are less than one mile from a highway/freeway

`more_than_12` the number of stations that have more than 12 chargers

`no_v2`the number of V2 chargers

`no_urban` the number of urban chargers

`no_v3` the number of V3 chargers

`no_v4` the number of V4 chargers

`total` total number of chargers

## How The Data Was Collected

The data from this data set was gathered via various many online sources. When deciding on location, we wanted to focus on a state that we were familiar with so we chose California. When collecting data, we chose to only collect data from counties that had a population greater than 500,000. The reason we made this decision was because we wanted to focus more on higher populated counties. Our data set contains 16 counties and will contain 296 observations.

Counties have a combination of towns, unincorporated cities, and incorporated cities. In our data set, we are ONLY considering incorporated cities.

As for how the data was collected, the parameters:

`population`, `land_size`, `medium_income`, and `per_capita_income` were scraped from the [US Census](https://www.census.gov/quickfacts/). The following cities: Maricopa, Isleton, Indian Wells, Del Mar, Monte Sereno, Brisbane, Vernon, Rolling Hills, Irwindale, Industry, Hidden Hills, Bradbury, and Avalon were not found on the US Census so we scraped the demopgraphic data from the [Census Reporter](https://censusreporter.org/).

`registered_voter`, `democratic`, and `republican` were scraped from [Report of Registration](https://www.sos.ca.gov/elections/report-registration/88day-primary-2022). This data comes from the California Secretary of State and was reported on March 11, 2022.

`no_stations`, `more_than_12`, `no_v2`, `no_urban`, `no_v3`, `no_v4`, and `total` were scraped from [Supercharge.io](https://supercharge.info/changes), a community based forum centered on Tesla Supercharger data world-wide. The data we collected from Supercharge.io was done on April 30, 2024.

`mile` was self obtained from Google Maps using the "Measure Distance" feature. This distance was measured from each Tesla Supercharger station to the nearest highway/freeway enterance.

## Loading the data set

Lets load the data set and view it.

```{r}
tesla <- read.csv("Tesla Dataset.csv")
tesla <- tesla %>%
  mutate(county = as.factor(county))
tesla
```

# Exploratory Data Analysis

To further analyze our data set, we will do a bit of exploratory data analysis.

## Distribution of number of stations

```{r}
tesla %>% 
  ggplot(aes(no_stations)) + 
  geom_bar(fill = "black") + 
  labs(title = "Distribution of Number of Stations")

tesla %>% 
  mutate(no_stations = as.factor(no_stations)) %>%
  mutate(no_stations = fct_lump_n(no_stations, n = 1, other_level = "1+ Stations",)) %>%
  ggplot(aes(no_stations)) + 
  geom_bar(fill = "black") + 
  labs(title = "Distribution of Number of Stations")
```

From the 2 graphs, we can see the distribution of the number of stations for each of our observations (cities). The first graph shows us the full distribution of all our observations. From this graph, we can see that the majority of our observations have 0 stations. For observations that do have stations, majority of them have only one charging station. The overall distribution is right skewed.

In the second graph, we have lumped observations with one or more station into one category. We can see that we have about equal observations of cities with 0 chargers and cities with at least one charging station. 

## Graph of Population vs Total Number of Chargers

```{r}
ggplot(tesla, aes(x = population, y = total)) +
  geom_point() +
  labs(title = "Population vs. Total Number of Chargers",
       x = "Population (2022)",
       y = "Total Number of Chargers")
```

In the Population v.s. Total Number of Chargers graph, we can see that majority of our observations are clustered to the bottom left. We can also see a slight positive correlation, however it is not a strong positive correlation. 

## Number of Stations v.s. Population by County

```{r}
ggplot(tesla, aes(fill=county, y=population, x=no_stations)) + 
    geom_bar(position="stack", stat="identity")
```

In this Number of Stations v.s. Population by County stacked bar chart we can see that 

```{r}
ggplot(tesla, aes(x=population, y=no_stations, col=county))+
  geom_point() 
```

```{r}
tesla_model <- tesla %>%
  select_if(is.numeric) %>%
  select(-no_v4)


tesla_model %>%  # none of our cities contain a v4 charger
  cor() %>%
  corrplot.mixed()
```

# Model Building

For our model recipe, we will be removing the following variables: `city`, `mile`, `more_than_12`, `no_v2`, `no_urban`, `no_v3`, `no_v4`, and total. We are doing this because we want to build a model that can predict the no_stations based on demographic data of each observation. This updated data set will be called `tesla_model`

```{r}
tesla_model <- tesla %>%
  select(-city) %>%
  select(-mile) %>%
  select(-more_than_12) %>%
  select(-no_v2) %>%
  select(-no_urban) %>%
  select(-no_v3) %>%
  select(-no_v4) %>%
  select(-total)

head(tesla_model)
```

## Data Split

We will first begin with splitting out data into training and testing data. The purpose of doing so is to build and train our predictive model on our training data and test the model on unseen data (the testing data). For this project, we will be using a 70/30 training to testing split.

```{r}
tesla_split <- initial_split(tesla_model, prop = 0.7, strata = no_stations)
tesla_train <- training(tesla_split)
tesla_test <- testing(tesla_split)
```

## Recipe

```{r}
tesla_recipe <- recipe(no_stations ~  county + population + land_size + 
                         medium_income + per_capita_income + registered_voter +
                         democratic + republican, data = tesla_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) 
```

## Model

```{r}
tesla_folds <- vfold_cv(tesla_train, v = 5, strata = no_stations)
```

1. Setting up the model
```{r}
# Linear Model
lm_model <- linear_reg() %>% 
  set_engine("lm")

# KNN
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

# Random Forest
random_forest_model <- rand_forest(mtry=tune(),
                                 trees = tune(),
                                 min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Elastic Net
elastic_model <- linear_reg(mixture = tune(), 
    penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
```

2. Creating Workflow
```{r}
# Linear Model
lm_workflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(tesla_recipe)

# KNN
knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(tesla_recipe)

# Random Forest
random_forest_workflow <- workflow() %>%
  add_model(random_forest_model) %>%
  add_recipe(tesla_recipe)
```

3. Creating Grid
```{r}
# KNN
knn_grid <- grid_regular(neighbors(range = c(1,10)), levels = 10)

# Random Forest
rf_grid <- grid_regular(mtry(range = c(1, 6)), 
                        trees(range = c(200, 600)),
                        min_n(range = c(10, 20)),
                        levels = 5)
```

4. Tune Model
```{r}
# KNN 
knn_tune <- tune_grid(knn_workflow, resamples = tesla_folds, grid = knn_grid)

# Random Forest
tune_rf <- tune_grid(
  object = random_forest_workflow, resamples = tesla_folds, grid = rf_grid)

```

5. Collect metrics
```{r}
# Linear Model
lm_fit <- fit_resamples(lm_workflow, resamples = tesla_folds)
lm_rmse <- collect_metrics(lm_fit)

# KNN
knn_rmse <- collect_metrics(knn_tune) %>%
  filter(.metric=='rmse')%>%
  arrange(mean)

# Random Forest
rf_rmse <- collect_metrics(tune_rf) %>%
  filter(.metric=='rmse') %>%
  arrange(mean)

lm_mean <- head(lm_rmse, n=1)
knn_mean <- head(knn_rmse, n=1)
rf_mean <- head(rf_rmse, n=1)

lm_mean
knn_mean
rf_mean
```

5. Fitting the model
```{r}
# Creating a tibble of all the models and their RMSE
final_compare_tibble <- tibble(Model = c("Linear Regression", "K Nearest Neighbors", "Random Forest"), RMSE = c(lm_mean$mean, knn_mean$mean, rf_mean$mean))

ggplot(final_compare_tibble, aes(x=reorder(Model, +RMSE), y=RMSE)) +
  geom_bar(stat = "identity", aes(fill = Model)) +
  scale_fill_manual(values = c("blue", "red", "orange")) +
  theme(legend.position = "none") +
  labs(title = "Comparing RMSE by Model", x = "Models")

# Arranging by lowest RMSE
final_compare_tibble <- final_compare_tibble %>% 
  arrange(RMSE)

final_compare_tibble
```
When comparing the models on the cross-validated data, we can see that the Random Forest model performed the best with a RMSE of 1.055483. 

```{r}
autoplot(tune_rf, metric = "rmse")
```


```{r}
best_rf <- select_best(tune_rf, metric = 'rmse')
rf_final_workflow_train <- finalize_workflow(random_forest_workflow, best_rf)
rf_final_fit_train <- fit(rf_final_workflow_train, data = tesla_train)
```

```{r}
tesla_tibble <- predict(rf_final_fit_train, new_data = tesla_test %>% select(-no_stations))
tesla_tibble <- bind_cols(tesla_tibble, tesla_test %>% select(no_stations))
```

```{r}
tesla_metric <- metric_set(rmse)

# Collecting the rmse of the model on the testing data
tesla_tibble_metrics <- tesla_metric(tesla_tibble, truth = no_stations, estimate = .pred)
tesla_tibble_metrics
```

