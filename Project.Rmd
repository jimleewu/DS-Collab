---
title: "Tesla Supercharger Analysis"
author: "Jim-Lee Wu and Fion Huang"
subtitle: 
output:
  pdf_document:
    number_sections: true
  html_document:
    number_sections: true
  word_document: default
---

```{r}
library(dplyr)
library(tidyverse)
library(tidymodels)
library(rsample)
library(corrplot)
library(ranger)
library(glmnet)
library(kknn)
library(xgboost)
library(ggplot2)
```

# Introduction

There has been an increasing rise in popularity in electronic vehicles (EVs) throughout the past decade for its environmental, economic, and performance benefits. With so many EVs filling the streets, a need for a proper EV charging infrastructure is needed. Just like gasoline cars, EV drivers require charging stations where they can recharge when going long distances.

# Goal Of This Project

The goal of this project is to develop a prediction model that is able to predict whether or not a city has a Tesla Supercharger given demographic data about the city. In this project, we will explore data scraping as we will be collecting our own data. After collecting our data, we will utilize different machine learning models to develop an adequate perdiction model and data analysis methods to analyze our data set and performance of our models. 

# Dataset

The data set that we will be working on in this project is called `tesla`. The data consist of all cities from highly populated counties in California. The counties in our data set consist of Alameda, Contra Costa, Fresno, Kern, Los Angeles, Orange, Riverside, Sacramento, San Diego, San Francisco, San Joaquin, San Mateo, Santa Barbara, Santa Clara, Stanislaus, and Ventura. We collected demographic data and Tesla Supercharging data for each city in the before mention counties.

## Parameters

`city` City

`county` County of City

`population` 2022 Population

`land_size` 2020 Land area in square miles

`medium_income` medium household income in 2018-2022 (in 2022 dollars)

`per_capita_income` per_capita_income: mean income of every person in 2018-2022 (in 2022 dollars)

`registered_voter` the number of registered voters (March 11, 2022)

`democratic` the number of registered voters declared as democratic (March 11, 2022)

`republican`the number of registered voters declared as republican (March 11, 2022)

`no_stations` the number of Tesla Supercharger stations. This will later be changed to 0 if there are 0 stations and 1 if there is at least one station.

`mile` the number of stations that are less than one mile from a highway/freeway

`more_than_12` the number of stations that have more than 12 chargers

`no_v2`the number of V2 chargers

`no_urban` the number of urban chargers

`no_v3` the number of V3 chargers

`no_v4` the number of V4 chargers

`total` total number of chargers

## How The Data Was Collected

The data from this data set was gathered via various many online sources. When deciding on location, we wanted to focus on a state that we were familiar with so we chose California. When collecting data, we chose to only collect data from counties that had a population greater than 500,000. The reason we made this decision was because we wanted to focus more on higher populated counties. Our data set contains 16 counties and will contain 296 observations.

Counties have a combination of towns, unincorporated cities, and incorporated cities. In our data set, we are ONLY considering incorporated cities.

As for how the data was collected, the parameters:

`population`, `land_size`, `medium_income`, and `per_capita_income` were scraped from the [US Census](https://www.census.gov/quickfacts/). The following cities: Maricopa, Isleton, Indian Wells, Del Mar, Monte Sereno, Brisbane, Vernon, Rolling Hills, Irwindale, Industry, Hidden Hills, Bradbury, and Avalon were not found on the US Census so we scraped the demopgraphic data from the [Census Reporter](https://censusreporter.org/).

`registered_voter`, `democratic`, and `republican` were scraped from [Report of Registration](https://www.sos.ca.gov/elections/report-registration/88day-primary-2022). This data comes from the California Secretary of State and was reported on March 11, 2022.

`no_stations`, `more_than_12`, `no_v2`, `no_urban`, `no_v3`, `no_v4`, and `total` were scraped from [Supercharge.io](https://supercharge.info/changes), a community based forum centered on Tesla Supercharger data world-wide. The data we collected from Supercharge.io was done on April 30, 2024.

`mile` was self obtained from Google Maps using the "Measure Distance" feature. This distance was measured from each Tesla Supercharger station to the nearest highway/freeway enterance.

## Loading the data set

Lets load the data set and view it.

```{r}
tesla <- read.csv("Tesla Dataset.csv")
tesla <- tesla %>%
  mutate(county = as.factor(county)) %>%
  mutate(no_stations = as.factor(no_stations))
tesla
```

# Exploratory Data Analysis

To further analyze our data set, we will do a bit of exploratory data analysis.

## Distribution of number of stations

```{r}
tesla %>% 
  ggplot(aes(no_stations)) + 
  geom_bar(fill = "red") + 
  labs(title = "Distribution of Number of Stations")

tesla %>% 
  mutate(no_stations = as.factor(no_stations)) %>%
  mutate(no_stations = fct_lump_n(no_stations, n = 1, other_level = "1+ Stations",)) %>%
  ggplot(aes(no_stations)) + 
  geom_bar(fill = "red") + 
  labs(title = "Distribution of Number of Stations")
```

From the 2 graphs, we can see the distribution of the number of stations for each of our observations (cities). The first graph shows us the full distribution of all our observations. From this graph, we can see that the majority of our observations have 0 stations. For observations that do have stations, majority of them have only one charging station. The overall distribution is right skewed.

In the second graph, we have lumped observations with one or more station into one category. We can see that we have about equal observations of cities with 0 chargers and cities with at least one charging station.

## Graph of Population vs Total Number of Chargers

```{r}
ggplot(tesla, aes(x = population, y = total)) +
  geom_point() +
  labs(title = "Population vs. Total Number of Chargers",
       x = "Population (2022)",
       y = "Total Number of Chargers")
```

In the Population v.s. Total Number of Chargers graph, we can see that majority of our observations are clustered to the bottom left. We can also see a slight positive correlation, however it is not a strong positive correlation.

## Graph of County vs. Tesla Supercharging Station Presence

```{r, fig.width= 5, fig.height=5}
ggplot(tesla, aes(x = county, fill = no_stations)) + 
  geom_bar(position = "dodge") + 
  theme_minimal() + 
  labs(title = "County vs. Tesla Supercharging Station Presence") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

tesla %>% 
  mutate(no_stations = as.factor(no_stations)) %>%
  mutate(no_stations = fct_lump_n(no_stations, n = 1, other_level = "1+ Stations",)) %>%
  ggplot(aes(x = county, fill = no_stations)) + 
  geom_bar(position = "dodge") + 
  theme_minimal() + 
  labs(title = "County vs. Tesla Supercharging Station Presence") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

In this Number of Stations v.s. Population by County stacked bar chart we can see that

```{r}
ggplot(tesla, aes(x=population, y=no_stations, col=county))+
  geom_point() 
```

In this graph, we can

```{r}
tesla$democratic_proportion <- tesla$democratic / tesla$registered_voter
tesla$republican_proportion <- tesla$republican / tesla$registered_voter

ggplot(tesla, aes(x = no_stations, y = democratic_proportion)) + 
  geom_boxplot() + 
  theme_minimal() + 
  labs(title = "Democratic Proportion vs. Tesla Supercharging Station Presence")

ggplot(tesla, aes(x = no_stations, y = republican_proportion)) + 
  geom_boxplot() + 
  theme_minimal() + 
  labs(title = "Republican Proportion vs. Tesla Supercharging Station Presence")

```


# Preparing Our Data Set For Model Building 

For our model recipe, we will be removing the following variables: `city`, `mile`, `more_than_12`, `no_v2`, `no_urban`, `no_v3`, `no_v4`, and total. We are doing this because we want to build a model that can predict the no_stations based on demographic data of each observation. This updated data set will be called `tesla_model`

```{r}
tesla_model <- tesla %>%
  mutate(no_stations = fct_lump_n(no_stations, n = 1, other_level = "1")) %>%
  select(-city) %>%
  select(-mile) %>%
  select(-more_than_12) %>%
  select(-no_v2) %>%
  select(-no_urban) %>%
  select(-no_v3) %>%
  select(-no_v4) %>%
  select(-total)

head(tesla_model)
```

```{r}
corr_model <- tesla_model %>%
  mutate(no_stations = as.numeric(no_stations)) %>%
  select_if(is.numeric) 

corr_model %>%
  cor() %>%
  corrplot.mixed()
```

# Model Building

We will now start building the models with our recipe we created earlier and we will be tuning our models with the folds we mentioned in the previous step. After tuning, we will examine the ROC_AUC metric across all of our models to see which model performed the best on the training data. We are using the ROC_AUC because it provides a single scalar value that quantifies the overall performance of the model, making it easy to interpret and compare across different models. The ROC curve will also provide a visual representation of the model's performance across all classification thresholds. We will be creating three models: K-Nearest Neighbors, Random Forest, and Elastic Net Multi-Nominal Regression.

## Explanation of each model

A quick explantion of each model we will be building.

### K - Nearest Neightbors 

The k-Nearest Neighbors (k-NN) classification model is a straightforward and intuitive machine learning algorithm used for classifying data points based on the categories of their nearest neighbors. It operates by storing all the training data and does not involve a traditional training phase. When a new data point needs to be classified, the algorithm calculates the distance between this point and all points in the training dataset using a distance metric such as Euclidean distance. It then identifies the k nearest neighbors to the query point, where k is a user-defined constant. The new data point is classified based on a majority vote among the k nearest neighbors, meaning it is assigned the class that is most common among these neighbors. k-NN can be computationally intensive and sensitive to irrelevant features and high-dimensional data. Despite its limitations, k-NN remains a powerful tool for both classification and regression tasks, particularly when interpretability and simplicity are crucial.

### Random Forest

Random Forest is a versatile ensemble learning technique that utilizes the collective predictions of multiple decision trees to produce robust and accurate models. Each decision tree in the forest is trained independently on a random subset of the training data and a random subset of the features. This randomness helps to decorrelate the trees, reducing the risk of overfitting and improving generalization. During prediction, the results of individual trees are aggregated, typically through averaging for regression tasks or voting for classification tasks, to produce the final prediction. Random Forest models are known for their robustness to noisy data, ability to handle high-dimensional datasets, and automatic feature selection capabilities, making them widely applicable across various domains.

### Elastic Net Multi-Nominal Regression

Elastic Net Multi-Nominal Regression is a statistical technique that extends the traditional multinomial logistic regression model by incorporating both L1 (Lasso) and L2 (Ridge) regularization penalties. This hybrid regularization technique helps address issues such as multicollinearity and overfitting by penalizing the absolute size of the coefficients (L1 penalty) and their squared magnitude (L2 penalty). Elastic Net regression is particularly useful when dealing with high-dimensional datasets with many correlated predictors, as it encourages sparsity while also providing some level of stability and robustness. Elastic Net Multi-Nominal Regression is commonly used in machine learning and predictive modeling tasks where the goal is to classify observations into multiple categories based on a set of predictor variables.

## Data Split

We will first begin with splitting out data into training and testing data. The purpose of doing so is to build and train our predictive model on our training data and test the model on unseen data (the testing data). For this project, we will be using a 70/30 training to testing split.

```{r}
tesla_split <- initial_split(tesla_model, prop = 0.7, strata = no_stations)
tesla_train <- training(tesla_split)
tesla_test <- testing(tesla_split)
```

## Recipe

We will now start creating our recipe. We will create a universal recipe that can be used by all of our models . The recipe will include 8 predictors from our training tesla data set (tesla_train). These variables are `county`, `population`, `land_size`, `medium_income`, `per_capita_income`, `registered_voter`, `demoractic`, and `republican`. We will step_dummy our categorical predictors (county). We will also step_zv to clear any instances of zero variance. Lastly, we will scale and center all our predictors using step_normalize(). 

```{r}
tesla_recipe <- recipe(no_stations ~  county + population + land_size + 
                         medium_income + per_capita_income + registered_voter +
                         democratic + republican, data = tesla_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) 
```

## K-Fold Cross Validation

We will employ a k-fold stratified cross-validation approach with 5 folds, where the data set is divided into 5 equal-sized folds. During each iteration, one fold is reserved as the testing set, while the remaining 4 folds (k-1) collectively form the training set. This process is repeated until each fold has served as the testing set exactly once. Subsequently, the model is trained on each training set and evaluated on the corresponding testing set. The average accuracy across all folds is then computed to gauge performance. Metrics like ROC_AUC, accuracy, and standard error can be reviewed to determine performace. This methodology offers a more robust estimate of testing accuracy compared to training on the entire dataset, as it reduces variability by averaging results over multiple iterations.

```{r}
tesla_folds <- vfold_cv(tesla_train, v = 5, strata = no_stations)
```

## Creating The Models

Creating the models for K-Nearest Neighbors, Random Forest, and Elastic Net Multi-Nominal Regression.

```{r}
# KNN
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Random Forest
random_forest_model <- rand_forest(mtry=tune(),
                                 trees = tune(),
                                 min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# Elastic Net
elastic_model <- multinom_reg(mixture = tune(), 
    penalty = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")
```

## Creating the Workflows

Creating the workflows for our models.

```{r}
# KNN
knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(tesla_recipe)

# Random Forest
random_forest_workflow <- workflow() %>%
  add_model(random_forest_model) %>%
  add_recipe(tesla_recipe)

# Elastic Net
elastic_workflow <- workflow() %>%
  add_model(elastic_model) %>%
  add_recipe(tesla_recipe)

```

## Tuning Grid

We will now create the tuning grids to specify the ranges for our parameters that the model will be tuning and how many levels.

For K- Nearest Neighbors we will be tuning neighbors

For Random Forest we will be tuning mtry: the number of randomly selected features considered at each split when constructing each decision tree in the forest, trees: the number of decision trees to include, and min_n: the minimum number of observations required to split a node further in each decision tree.

For Elastic Net Multi-Nominal Regression we will be tuning penalty: how much the model is penalized for having large coefficients. and mixture: the balance between two types of penalties: L1 (Lasso) and L2 (Ridge)

```{r}
# KNN
knn_grid <- grid_regular(neighbors(range = c(1,10)), levels = 10)

# Random Forest
rf_grid <- grid_regular(mtry(range = c(1, 6)), 
                        trees(range = c(200, 600)),
                        min_n(range = c(10, 20)),
                        levels = 5)

en_grid <- grid_regular(penalty(range = c(0, 1), trans = identity_trans()),
                        mixture(range = c(0, 1)), levels = 10)
```

## Tuning Hyperparameters

We will tune all three of our models and save them to their own rda file. We can load them back in anytime for faster load times and so we do not have to tune the models every time we want to see the results.

```{r}
# KNN 
knn_tune <- tune_grid(knn_workflow, resamples = tesla_folds, grid = knn_grid)

# Random Forest
tune_rf <- tune_grid(
  object = random_forest_workflow, resamples = tesla_folds, grid = rf_grid)

# Elastic Net
tune_elastic <- tune_grid(
  object = elastic_workflow, resamples = tesla_folds, grid = en_grid)

save(knn_tune, file = "knn_tune.rda")
save(tune_rf, file = "tune_rf.rda")
save(tune_elastic, file = "tune_elastic.rda")
```

## Loading in our tuned models

```{r}
load("knn_tune.rda")
load("tune_rf.rda")
load("tune_elastic.rda")
```

# Model Results

## Obtaining ROC_AUC For Each Model

Lets collect the ROC_AUC metric of our tuned models and arranged them in decreasing mean since we are looking for the model with the highest ROC_AUC. We will then head the data frame to get the model with the highest ROC_AUC.

```{r}
# KNN
knn_roc_auc <- collect_metrics(knn_tune) %>%
  filter(.metric=='roc_auc')%>%
  arrange(desc(mean))

# Random Forest
rf_roc_auc <- collect_metrics(tune_rf) %>%
  filter(.metric=='roc_auc') %>%
  arrange(desc(mean))

# Elastic Net
elastic_roc_auc <- collect_metrics(tune_elastic) %>%
  filter(.metric=='roc_auc') %>%
  arrange(desc(mean))

knn_mean <- head(knn_roc_auc, n=1)
rf_mean <- head(rf_roc_auc, n=1)
elastic_mean <- head(elastic_roc_auc, n=1)

knn_mean
rf_mean
elastic_mean
```

## Bar Chart For Comparisson

We will now create a bar chart to compare the highest ROC_AUC of all the models

```{r}
# Creating a tibble of all the models and their RMSE
final_compare_tibble <- tibble(Model = c("K Nearest Neighbors", "Random Forest", "Elastic Net"), ROC_AUC = c(knn_mean$mean, rf_mean$mean, elastic_mean$mean))

ggplot(final_compare_tibble, aes(x=reorder(Model, -ROC_AUC), y=ROC_AUC)) +
  geom_bar(stat = "identity", aes(fill = Model)) +
  scale_fill_manual(values = c("blue", "red", "orange")) +
  theme(legend.position = "none") +
  labs(title = "Comparing ROC_AUC by Model", x = "Models")

# Arranging by lowest RMSE
final_compare_tibble <- final_compare_tibble %>% 
  arrange(desc(ROC_AUC))

final_compare_tibble
```

When comparing the models on the cross-validated data, we can see that the Random Forest model performed the best with a ROC_AUC of 0.7957143

```{r}
autoplot(tune_rf, metric = "roc_auc")
```

We can see from the autoplot of the tuned random forest the the the min_n (Minimal Node Size) had minor effect on the results. Across all minimal node sizes, the ROC_AUC metric was between 0.76 and 0.80. The lower the mtry values (# Randomly Selected Predictors) were, the higher the ROC_AUC and starts to decrease as mtry increases. The model performs the best at lower numbers of randomly selected predictors. The number of trees does seem to effect the results as each graph shows how the amount of trees affect the performance of the model. 

## Fit Best Model To Our Training Data

We will now take the best model from the tuned random forest model and fit it to the training data. This will train that our best model one more time on the entire training data set.

```{r}
best_rf <- select_best(tune_rf, metric = 'roc_auc')
rf_final_workflow_train <- finalize_workflow(random_forest_workflow, best_rf)
rf_final_fit_train <- fit(rf_final_workflow_train, data = tesla_train)
```

## Testing The Model On Testing Data

We wil; finally test our trained random forest model on our unseen testing data. 

```{r}
tesla_predict <- augment(rf_final_fit_train, new_data = tesla_test)
tesla_predict %>%
  roc_auc(truth = no_stations, .pred_0)
```
```{r}
roc_curve(tesla_predict, truth = no_stations, .pred_0) %>% 
  autoplot()

conf_mat(tesla_predict, truth = no_stations, 
         .pred_class) %>% 
  autoplot(type = "heatmap")
```
Based on the confusion matrix our model correctly predicted:

37 cities with `no_stations` = 0 out of 46 cities correctly 

27 cities with `no_stations` = 1 out of 44 cities correctly

# Conclusion 

